{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv, glob, random, string, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col,isnan,when,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    csvfile = open(path, newline='')\n",
    "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    return list(reader)\n",
    "\n",
    "def write_csv(path, data):\n",
    "    f = open(path, 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)\n",
    "    f.close()\n",
    "\n",
    "def read_json(path):\n",
    "    return json.load(open(path))\n",
    "\n",
    "def write_json(path, data):\n",
    "    file = open(path, \"w+\")\n",
    "    file.write(json.dumps(data)) \n",
    "    file.close()\n",
    "\n",
    "def get_subpaths(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "def generate_cat(size):\n",
    "    return ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(size))\n",
    "\n",
    "def write_tfrecord(dir, df):\n",
    "    df.write.format(\"tfrecords\").option(\"recordType\", \"Example\").save(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.dirname(os.getcwd()) + \"/\"\n",
    "ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = ROOT_DIR + \"resources/spark/\"\n",
    "os.environ['SPARK_HOME']\n",
    "connector = ROOT_DIR + \"resources/ecosystem/spark/spark-tensorflow-connector/target/spark-tensorflow-connector_2.12-1.11.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ROOT_DIR + \"data/criteo/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = DATA_DIR + \"train/\"\n",
    "# TARGET_DIR = DATA_DIR + \"sample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_PATH = DATA_DIR + \"schema.json\"\n",
    "PART_DIRS = get_subpaths(TARGET_DIR + \"part*/\")\n",
    "PART_DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pCTR\").\\\n",
    "    config('spark.jars', connector).\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType.fromJson(json.load(open(SCHEMA_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [\"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\", \"i11\", \"i12\", \"i13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric(df, total, stats, num_features, normalizer = \"minmax\"):\n",
    "    def bring_stat(stats, feature):\n",
    "        keys = [\"min\", \"max\", \"avg\", \"median\", \"mode\", \"nulls\", \"stddev\"]\n",
    "        stat = {}\n",
    "        for key in keys:\n",
    "            stat[key]= stats[\"{}({})\".format(key, feature)]\n",
    "        return stat \n",
    "\n",
    "    for feature in num_features:\n",
    "        stat = bring_stat(stats, feature)\n",
    "        # if mode is not 0, safe to assume missing = 0. (assuming non-negative data)\n",
    "        if stat[\"mode\"] != 0:\n",
    "            df = df.fillna(value = 0, subset = [feature])\n",
    "        else: # else if mode is 0, then:\n",
    "            null_ratio = stat[\"nulls\"] / total\n",
    "            if null_ratio < 0.1: # if null ratio is low, safe to assume missing = 0\n",
    "                df = df.fillna(value = 0, subset = [feature])\n",
    "            elif null_ratio > 0.6: # if null percentage is high: use mean\n",
    "                df = df.fillna(value = stat[\"avg\"], subset = [feature])\n",
    "            else:  # if null percentage is mid: use median\n",
    "                df = df.fillna(value = stat[\"median\"], subset = [feature])\n",
    "        # normalizer\n",
    "        if normalizer == \"minmax\": \n",
    "            df = df.withColumn(feature, (col(feature) - stat['min']) / (stat[\"max\"] - stat[\"min\"]))\n",
    "        elif normalizer == \"standard\":\n",
    "            df = df.withColumn(feature, (col(feature) - stat['avg']) / stat['stddev'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\", \"c9\", \"c10\", \"c11\", \"c12\", \"c13\", \"c14\", \"c15\", \"c16\", \"c17\", \"c18\", \"c19\", \"c20\", \"c21\", \"c22\", \"c23\", \"c24\", \"c25\", \"c26\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical(df, vocab_dir, cat_features):\n",
    "    for feature in cat_features:\n",
    "        json_path = vocab_dir + \"{}/count.json\".format(feature)\n",
    "        null_feature = read_json(json_path)[\"null_feature\"]\n",
    "        # go through the column and mark null as the new feature.\n",
    "        df = df.fillna(value = null_feature, subset = [feature])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part_dir in PART_DIRS:\n",
    "    stats = read_json(part_dir + \"stats/stats.json\")\n",
    "    total = read_json(part_dir + \"count/count.json\")[\"total\"]\n",
    "\n",
    "    data_path = get_subpaths(part_dir + \"raw/*.txt\")[0]\n",
    "    df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").schema(schema).csv(data_path)\n",
    "\n",
    "    df = numeric(df, total, stats, num_features, normalizer = \"standard\")\n",
    "    df = categorical(df, part_dir + \"vocabs/\", cat_features)\n",
    "    write_tfrecord(part_dir + \"data/\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81efb4751146ee777dacf2d48a0600c0d774437cf4ee3d204a1816da91ac0901"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('spark_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
