{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json, os, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, when, array, concat, size\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.functions import vector_to_array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.getcwd()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "DATA_DIR = ROOT_DIR + \"/data/criteo\"\n",
    "ECOSYSTEM_DIR = ROOT_DIR + \"/resources/ecosystem\"\n",
    "CONNECTOR_DIR = ECOSYSTEM_DIR + \"/spark/spark-tensorflow-connector/target\"\n",
    "# TRAIN_PATH = DATA_DIR + \"/full/train.txt\"\n",
    "TRAIN_PATH = DATA_DIR + \"/part/train/sample.txt\"\n",
    "SCHEMA_PATH = DATA_DIR + \"/full/schema.json\"\n",
    "CACHE_DIR = DATA_DIR + \"/cache/xdeepfm\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "spark = SparkSession.builder.appName(\"pCTR\").\\\n",
    "    config('spark.jars', CONNECTOR_DIR + \"/spark-tensorflow-connector_2.12-1.11.0.jar\").\\\n",
    "    getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/user/juhochoi/pctr-env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/01 18:03:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "schema = StructType.fromJson(json.load(open(SCHEMA_PATH)))\n",
    "df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").schema(schema).csv(TRAIN_PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "total = df.count()\n",
    "nulls = {}\n",
    "for i in df.columns[1:]:\n",
    "    nulls[i] = int(df.filter(col(i).isNull()).count() / total * 10000) / 100\n",
    "to_drop, to_transform, to_fill = [], [], []\n",
    "for feature in nulls:\n",
    "    if nulls[feature] > 70:\n",
    "        to_drop.append(feature)\n",
    "    elif nulls[feature] > 40:\n",
    "        to_transform.append(feature)\n",
    "    else:\n",
    "        to_fill.append(feature)\n",
    "to_fill_int = list(filter(lambda x : 'i' in x, to_fill))\n",
    "to_fill_cat = list(filter(lambda x : 'c' in x, to_fill))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "stats = {}\n",
    "described = df.select(to_fill_int).describe().collect()\n",
    "for feature in to_fill_int:\n",
    "    stats[feature] = {}\n",
    "    stats[feature][\"count\"] = int(described[0][feature])\n",
    "    stats[feature][\"mean\"] = round(float(described[1][feature]), 2)\n",
    "    stats[feature][\"stddev\"] = round(float(described[2][feature]), 2)\n",
    "    stats[feature][\"min\"] = int(described[3][feature])\n",
    "    stats[feature][\"q1\"], stats[feature][\"q2\"], stats[feature][\"q3\"] \\\n",
    "        = df.approxQuantile(feature, [0.25, 0.50, 0.75], 0)\n",
    "    stats[feature][\"max\"] = int(described[4][feature])\n",
    "    iqr = stats[feature]['q3'] - stats[feature]['q1']\n",
    "    stats[feature]['lower'] = stats[feature]['q1'] - (iqr * 1.5)\n",
    "    stats[feature]['upper'] = stats[feature]['q3'] + (iqr * 1.5)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(\"balancing...\")\n",
    "# balance\n",
    "label_count = df.groupBy(\"label\").count().withColumn(\"ratio\", \\\n",
    "    (col(\"count\") / total)).orderBy('label')\n",
    "ratio = label_count.collect()[1]['ratio'] / label_count.collect()[0]['ratio']\n",
    "clicked = df.filter(col(\"label\") == 1)\n",
    "unclicked = df.filter(col(\"label\") == 0).sample(False, ratio)\n",
    "df = clicked.union(unclicked)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "balancing...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(\"dropping...\")\n",
    "# drop\n",
    "df = df.drop(*to_drop) # * : for each"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dropping...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(\"transforming...\")\n",
    "# transform to boolean\n",
    "for feature in to_transform:\n",
    "    df = df.withColumn(feature, when(df[feature].isNull(), 0).otherwise(1).cast('string'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "transforming...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(\"filling numerical features...\")\n",
    "for feature in to_fill_int:\n",
    "    st = stats[feature]\n",
    "    # fill nulls with median and ceil negative values\n",
    "    df = df.withColumn(feature, when(col(feature).isNull(), st['q2']).\\\n",
    "        when(col(feature) < 0, 0).otherwise(col(feature)))\n",
    "    # replace outliers with median (again, mean is skewed)\n",
    "    df = df.withColumn(feature, when((col(feature) > st['upper']) | \\\n",
    "        (col(feature) < st['lower']), st['q2']).otherwise(col(feature)))\n",
    "    # standardize (z-scoring)\n",
    "    df = df.withColumn(feature, (col(feature) - st['mean']) / st['stddev'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "filling numerical features...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# fill nulls with mode for categorical features\n",
    "print(\"filling categorical features...\")\n",
    "label = df[\"label\"]\n",
    "clicked = df.filter(label == 1)\n",
    "unclicked = df.filter(label == 0)\n",
    "for feature in to_fill_cat:\n",
    "    clicked_mode = clicked.filter(col(feature).isNotNull()).groupBy(feature).count().\\\n",
    "        orderBy(\"count\", ascending=False).first()[0]\n",
    "    unclicked_mode = unclicked.filter(col(feature).isNotNull()).groupBy(feature).count().\\\n",
    "        orderBy(\"count\", ascending=False).first()[0]\n",
    "    df = df.withColumn(feature, when(col(feature).isNull() & (label == 1), clicked_mode).\\\n",
    "        when(col(feature).isNull() & (label == 0), unclicked_mode).otherwise(col(feature)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "filling categorical features...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "numeric_columns, categorical_columns = [], []\n",
    "for field in df.schema.fields:\n",
    "    if field.name == 'label':\n",
    "        continue\n",
    "    if str(field.dataType) in ('IntegerType', 'DoubleType'):\n",
    "        numeric_columns.append(field.name)\n",
    "    elif str(field.dataType) in ('StringType'):\n",
    "        categorical_columns.append(field.name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "numeric_columns = natural_sort(numeric_columns)\n",
    "categorical_columns = natural_sort(categorical_columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print(\"encoding categorical feature columns...\")\n",
    "original_cols = categorical_columns\n",
    "indexed_cols = [s + \"_indexed\" for s in categorical_columns]\n",
    "stringIndexer = StringIndexer(inputCols = original_cols, outputCols = indexed_cols)\n",
    "df = stringIndexer.fit(df).transform(df)\n",
    "# one-hot encoding features\n",
    "encoded_cols = [s + \"_vec\" for s in categorical_columns]\n",
    "ohe = OneHotEncoder(inputCols = indexed_cols, outputCols = encoded_cols, dropLast = False)\n",
    "df = ohe.fit(df).transform(df)\n",
    "df = df.drop(*original_cols).drop(*indexed_cols)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "encoding categorical feature columns...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "cardinalities = dict(zip(numeric_columns, [1] * len(numeric_columns)))\n",
    "for feature, encoded in zip(original_cols, encoded_cols):\n",
    "    df = df.withColumnRenamed(encoded, feature).withColumn(feature, vector_to_array(feature))\n",
    "    cardinalities[feature] = df.select(size(feature).alias(\"n\")).collect()[0]['n']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# concatenating cateogircal columns and creating dense numeric column\n",
    "df = df.select(['label'] + \n",
    "    [array(numeric_columns).alias('numeric'), concat(*categorical_columns).alias('categorical')])\n",
    "df = df.select(['label'] + \n",
    "    [concat(*['numeric', 'categorical']).alias(\"features\")])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df.show(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|[-0.2409524453764...|\n",
      "+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "metadata = {}\n",
    "metadata[\"numeric_fields\"] = numeric_columns\n",
    "metadata[\"categorical_fields\"] = categorical_columns\n",
    "metadata[\"num_fields\"] = len(categorical_columns) + len(numeric_columns)\n",
    "metadata[\"num_features\"] = sum(list(cardinalities.values()))\n",
    "metadata[\"cardinalities\"] = cardinalities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "if not os.path.isdir(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "metadata_file = open(CACHE_DIR + \"/metadata.json\", \"w+\")\n",
    "metadata_file.write(json.dumps(metadata)) \n",
    "metadata_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "k = 10\n",
    "weights = [1 / k] * k\n",
    "print(\"splitting data to {} folds...\".format(k))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "splitting data to 10 folds...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "for i, fold in enumerate(df.randomSplit(weights)): \n",
    "    path = CACHE_DIR + \"/fold{}.tfrecord\".format(i + 1)\n",
    "    fold.write.format(\"tfrecords\").option(\"recordType\", \"Example\").save(path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "test_path = CACHE_DIR + \"/fold{}.tfrecord\".format(1)\n",
    "test = spark.read.format(\"tfrecords\").option(\"recordType\", \"Example\").load(test_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "test.show(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-0.24997182, -0....|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('pctr-env': venv)"
  },
  "interpreter": {
   "hash": "7257144ef6fcc4ddff4bb2851add7cea84193c99c046c3ab01dd4120fefdcb15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}